{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "31ad6daa-f2d4-4b50-a681-88e01168e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "401dae66-b8ec-4b88-b94b-85bf26276945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pkey</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>source</th>\n",
       "      <th>status</th>\n",
       "      <th>url</th>\n",
       "      <th>image_url</th>\n",
       "      <th>disaster_type</th>\n",
       "      <th>report_dat</th>\n",
       "      <th>tags</th>\n",
       "      <th>...</th>\n",
       "      <th>ID_Desa</th>\n",
       "      <th>ID_Kec</th>\n",
       "      <th>ID_Kab</th>\n",
       "      <th>ID_Prov</th>\n",
       "      <th>Desa</th>\n",
       "      <th>Kecamatan</th>\n",
       "      <th>Kabupaten</th>\n",
       "      <th>Provinsi</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94941</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>00:03:02</td>\n",
       "      <td>grasp</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2042596d-b551-4ef2-803d-1ee964cf2278</td>\n",
       "      <td>https://images.petabencana.id/2042596d-b551-4e...</td>\n",
       "      <td>flood</td>\n",
       "      <td>{ \"report_type\": \"flood\", \"flood_depth\": 30 }</td>\n",
       "      <td>{ \"district_id\": \"3174\", \"local_area_id\": \"125...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.174020e+09</td>\n",
       "      <td>3174020.0</td>\n",
       "      <td>3174.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Duri Kepa</td>\n",
       "      <td>Kebon Jeruk</td>\n",
       "      <td>Jakarta Barat</td>\n",
       "      <td>Daerah Khusus Ibukota Jakarta</td>\n",
       "      <td>106.76866</td>\n",
       "      <td>-6.16978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95061</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>00:43:02</td>\n",
       "      <td>grasp</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>5a0ad45f-7020-43b7-882f-672487ebc6d8</td>\n",
       "      <td>https://images.petabencana.id/5a0ad45f-7020-43...</td>\n",
       "      <td>flood</td>\n",
       "      <td>{ \"points\": 1, \"flood_depth\": 80, \"report_type...</td>\n",
       "      <td>{ \"district_id\": null, \"local_area_id\": null, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.275061e+09</td>\n",
       "      <td>3275061.0</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Pejuang</td>\n",
       "      <td>Medan Satria</td>\n",
       "      <td>Bekasi</td>\n",
       "      <td>Jawa Barat</td>\n",
       "      <td>106.97870</td>\n",
       "      <td>-6.17944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95062</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>01:14:53</td>\n",
       "      <td>grasp</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>e065b40d-2084-48d6-b42f-1f98cb33e4cc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood</td>\n",
       "      <td>{ \"points\": 1, \"flood_depth\": 51, \"report_type...</td>\n",
       "      <td>{ \"district_id\": \"3172\", \"local_area_id\": \"178...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.172060e+09</td>\n",
       "      <td>3172060.0</td>\n",
       "      <td>3172.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Cipinang Muara</td>\n",
       "      <td>Jatinegara</td>\n",
       "      <td>Jakarta Timur</td>\n",
       "      <td>Daerah Khusus Ibukota Jakarta</td>\n",
       "      <td>106.89100</td>\n",
       "      <td>-6.23449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95063</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>01:15:08</td>\n",
       "      <td>grasp</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>7cb26903-b1c8-4be7-8b81-7014c8a83fbd</td>\n",
       "      <td>https://images.petabencana.id/7cb26903-b1c8-4b...</td>\n",
       "      <td>flood</td>\n",
       "      <td>{ \"points\": 2, \"flood_depth\": 100, \"report_typ...</td>\n",
       "      <td>{ \"district_id\": null, \"local_area_id\": null, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.216061e+09</td>\n",
       "      <td>3216061.0</td>\n",
       "      <td>3216.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Mekarmukti</td>\n",
       "      <td>Cikarang Utara</td>\n",
       "      <td>Bekasi</td>\n",
       "      <td>Jawa Barat</td>\n",
       "      <td>107.16189</td>\n",
       "      <td>-6.30170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95064</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>01:18:03</td>\n",
       "      <td>grasp</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>570d2cb8-d5b6-404f-86ef-8a2a0d27f391</td>\n",
       "      <td>https://images.petabencana.id/570d2cb8-d5b6-40...</td>\n",
       "      <td>flood</td>\n",
       "      <td>{ \"points\": 1, \"flood_depth\": 15, \"report_type...</td>\n",
       "      <td>{ \"district_id\": \"3173\", \"local_area_id\": \"193...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.173060e+09</td>\n",
       "      <td>3173060.0</td>\n",
       "      <td>3173.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Cempaka Baru</td>\n",
       "      <td>Kemayoran</td>\n",
       "      <td>Jakarta Pusat</td>\n",
       "      <td>Daerah Khusus Ibukota Jakarta</td>\n",
       "      <td>106.86092</td>\n",
       "      <td>-6.16859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pkey        date      time source     status  \\\n",
       "0  94941  01/01/2020  00:03:02  grasp  confirmed   \n",
       "1  95061  01/01/2020  00:43:02  grasp  confirmed   \n",
       "2  95062  01/01/2020  01:14:53  grasp  confirmed   \n",
       "3  95063  01/01/2020  01:15:08  grasp  confirmed   \n",
       "4  95064  01/01/2020  01:18:03  grasp  confirmed   \n",
       "\n",
       "                                    url  \\\n",
       "0  2042596d-b551-4ef2-803d-1ee964cf2278   \n",
       "1  5a0ad45f-7020-43b7-882f-672487ebc6d8   \n",
       "2  e065b40d-2084-48d6-b42f-1f98cb33e4cc   \n",
       "3  7cb26903-b1c8-4be7-8b81-7014c8a83fbd   \n",
       "4  570d2cb8-d5b6-404f-86ef-8a2a0d27f391   \n",
       "\n",
       "                                           image_url disaster_type  \\\n",
       "0  https://images.petabencana.id/2042596d-b551-4e...         flood   \n",
       "1  https://images.petabencana.id/5a0ad45f-7020-43...         flood   \n",
       "2                                                NaN         flood   \n",
       "3  https://images.petabencana.id/7cb26903-b1c8-4b...         flood   \n",
       "4  https://images.petabencana.id/570d2cb8-d5b6-40...         flood   \n",
       "\n",
       "                                          report_dat  \\\n",
       "0      { \"report_type\": \"flood\", \"flood_depth\": 30 }   \n",
       "1  { \"points\": 1, \"flood_depth\": 80, \"report_type...   \n",
       "2  { \"points\": 1, \"flood_depth\": 51, \"report_type...   \n",
       "3  { \"points\": 2, \"flood_depth\": 100, \"report_typ...   \n",
       "4  { \"points\": 1, \"flood_depth\": 15, \"report_type...   \n",
       "\n",
       "                                                tags  ...       ID_Desa  \\\n",
       "0  { \"district_id\": \"3174\", \"local_area_id\": \"125...  ...  3.174020e+09   \n",
       "1  { \"district_id\": null, \"local_area_id\": null, ...  ...  3.275061e+09   \n",
       "2  { \"district_id\": \"3172\", \"local_area_id\": \"178...  ...  3.172060e+09   \n",
       "3  { \"district_id\": null, \"local_area_id\": null, ...  ...  3.216061e+09   \n",
       "4  { \"district_id\": \"3173\", \"local_area_id\": \"193...  ...  3.173060e+09   \n",
       "\n",
       "      ID_Kec  ID_Kab  ID_Prov            Desa       Kecamatan      Kabupaten  \\\n",
       "0  3174020.0  3174.0     31.0       Duri Kepa     Kebon Jeruk  Jakarta Barat   \n",
       "1  3275061.0  3275.0     32.0         Pejuang    Medan Satria         Bekasi   \n",
       "2  3172060.0  3172.0     31.0  Cipinang Muara      Jatinegara  Jakarta Timur   \n",
       "3  3216061.0  3216.0     32.0      Mekarmukti  Cikarang Utara         Bekasi   \n",
       "4  3173060.0  3173.0     31.0    Cempaka Baru       Kemayoran  Jakarta Pusat   \n",
       "\n",
       "                        Provinsi       long      lat  \n",
       "0  Daerah Khusus Ibukota Jakarta  106.76866 -6.16978  \n",
       "1                     Jawa Barat  106.97870 -6.17944  \n",
       "2  Daerah Khusus Ibukota Jakarta  106.89100 -6.23449  \n",
       "3                     Jawa Barat  107.16189 -6.30170  \n",
       "4  Daerah Khusus Ibukota Jakarta  106.86092 -6.16859  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"PetaBencana_Dataset - PetaBencana_Dataset.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "edee18b2-23c6-4ed4-b6d2-aab1e3137ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bencana alam yang dilaporkan: \n",
      "['flood' 'earthquake' 'wind' 'fire' 'volcano' 'haze']\n"
     ]
    }
   ],
   "source": [
    "print(\"Bencana alam yang dilaporkan: \")\n",
    "print(df[\"disaster_type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0cae202b-f4b4-4bdc-a773-6a4ad2f547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Akan dibuat model NLP, sehingga baris-baris di mana kolom \"text\" kosong tidak akan berguna\n",
    "df = df.dropna(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "89e91b62-706e-418f-b078-4a013fbc2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buat dataframe berdasarkan tipe bencana\n",
    "\n",
    "flood = df[df[\"disaster_type\"] == \"flood\"]\n",
    "earthquake = df[df[\"disaster_type\"] == \"earthquake\"]\n",
    "wind = df[df[\"disaster_type\"] == \"wind\"]\n",
    "fire = df[df[\"disaster_type\"] == \"fire\"]\n",
    "volcano = df[df[\"disaster_type\"] == \"volcano\"]\n",
    "haze = df[df[\"disaster_type\"] == \"haze\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5c23c9de-ebad-4dfe-8a48-28aa5d2217b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flood DataFrame memiliki 3630 observasi\n",
      "earthquake DataFrame memiliki 112 observasi\n",
      "wind DataFrame memiliki 57 observasi\n",
      "fire DataFrame memiliki 12 observasi\n",
      "volcano DataFrame memiliki 17 observasi\n",
      "haze DataFrame memiliki 2 observasi\n"
     ]
    }
   ],
   "source": [
    "print(f\"flood DataFrame memiliki {flood.shape[0]} observasi\")\n",
    "print(f\"earthquake DataFrame memiliki {earthquake.shape[0]} observasi\")\n",
    "print(f\"wind DataFrame memiliki {wind.shape[0]} observasi\")\n",
    "print(f\"fire DataFrame memiliki {fire.shape[0]} observasi\")\n",
    "print(f\"volcano DataFrame memiliki {volcano.shape[0]} observasi\")\n",
    "print(f\"haze DataFrame memiliki {haze.shape[0]} observasi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d51cd30e-2618-4361-bc37-bcde6c7aec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_sample = flood[[\"disaster_type\", \"text\"]].sample(100, random_state=123)\n",
    "\n",
    "earthquake_sample = earthquake[[\"disaster_type\", \"text\"]].sample(100, random_state=123)\n",
    "\n",
    "# \n",
    "train_set = pd.concat([flood_sample[:70], earthquake_sample[:70]])\n",
    "train_x = train_set[\"text\"]\n",
    "train_y = train_set[\"disaster_type\"]\n",
    "\n",
    "\n",
    "test_set = pd.concat([flood_sample[70:], earthquake_sample[70:]])\n",
    "test_x = test_set[\"text\"]\n",
    "test_y = test_set[\"disaster_type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6470ac-04e8-4bd1-b147-3ba868382886",
   "metadata": {},
   "source": [
    "# Bag Of Words #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a7410eb3-8113-4365-b939-9ab98c68a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8114863d-92bd-4aa3-a6f8-be69da929467",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "beb1d7d4-33ed-4977-8521-0d6ee8500fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier support vector machine\n",
    "clf_svm = svm.SVC()\n",
    "clf_svm.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b6faac2d-03f3-4ab6-928c-6857f6bb49b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.8166666666666667\n"
     ]
    }
   ],
   "source": [
    "test_x_vectors = vectorizer.transform(test_x)\n",
    "\n",
    "print(f\"Test set score: {np.mean([clf_svm.predict(test_x_vectors) == np.array(test_y)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937de604-e0d6-4136-9b41-20e357b5866d",
   "metadata": {},
   "source": [
    "# Bag Of Words #2\n",
    "this time with some settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "28962a3c-76f3-421b-840b-237168a07287",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)  # n_gram , 2 word phrases. binary?\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "21f7d63a-8b96-4191-8ad5-b4e245ce8968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier support vector machine\n",
    "clf_svm = svm.SVC(kernel=\"linear\")  # kernel = \"linear\" good for text\n",
    "clf_svm.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "84dae03d-28e1-478f-ae52-8e45e76e094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.8833333333333333\n"
     ]
    }
   ],
   "source": [
    "test_x_vectors = vectorizer.transform(test_x)\n",
    "\n",
    "print(f\"Test set score: {np.mean([clf_svm.predict(test_x_vectors) == np.array(test_y)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29b35d-3d0d-436b-8850-27554db73495",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fa6778-7bc1-408a-a770-7f25c9c8a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy (if haven't already)\n",
    "\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0d4a47e5-1552-4a39-af97-a44063b3671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7d58f598-ed2b-4a5f-a0a6-b78e9e21076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c7102056-ab64-43a5-870b-259c2f9dca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = [nlp(text) for text in train_x]\n",
    "train_x_word_vectors = [x.vector for x in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9aaa43fc-f1a4-48e7-9946-f1d3f7ba0f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_wv = svm.SVC(kernel=\"linear\")  # linear good for text\n",
    "clf_svm_wv.fit(train_x_word_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "78b7a8c8-c2cb-444a-9904-14c34716bba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.5166666666666667\n"
     ]
    }
   ],
   "source": [
    "test_docs = [nlp(text) for text in test_x]  # nlp(text), averages all words embedding \n",
    "test_x_word_vectors = [x.vector for x in test_docs]\n",
    "\n",
    "print(f\"Test set score: {np.mean([clf_svm_wv.predict(test_x_word_vectors) == np.array(test_y)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c2089-3b94-4ef2-8db0-e1095d0232d4",
   "metadata": {},
   "source": [
    "# Stopwords removal\n",
    "- removing unnecessary words to improve accuracy\n",
    "- https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba54e76-548b-4a0e-9089-4c5dfcca578d",
   "metadata": {},
   "source": [
    "# Bag of Words #3 (with stopwords removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bc4b3a76-3c2f-44e8-93d1-8accfd20b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"indonesian\")\n",
    "\n",
    "train_x_sr = []\n",
    "for phrase in train_x:\n",
    "    \n",
    "    words = word_tokenize(phrase)\n",
    "\n",
    "    stripped_phrase = []\n",
    "\n",
    "\n",
    "    for word in word_tokenize(phrase):\n",
    "        if word not in stop_words:\n",
    "            stripped_phrase.append(word)\n",
    "\n",
    "    stripped_phrase\n",
    "    phrase = \" \".join(stripped_phrase)\n",
    "    train_x_sr.append(phrase)\n",
    "\n",
    "# train_x_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a4c8b342-cb00-4723-a0b5-623644c99aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_x_vectors = vectorizer.fit_transform(train_x_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "29f20c03-bc07-403c-ac06-36483f112856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier support vector machine\n",
    "clf_svm = svm.SVC(kernel=\"linear\")  # kernel = \"linear\" good for text\n",
    "clf_svm.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6cebdda9-189b-4b45-9e15-f3aede049e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_sr = []\n",
    "for phrase in test_x:\n",
    "    \n",
    "    words = word_tokenize(phrase)\n",
    "\n",
    "    stripped_phrase = []\n",
    "\n",
    "\n",
    "    for word in word_tokenize(phrase):\n",
    "        if word not in stop_words:\n",
    "            stripped_phrase.append(word)\n",
    "\n",
    "    stripped_phrase\n",
    "    phrase = \" \".join(stripped_phrase)\n",
    "    test_x_sr.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7d4aa494-4a36-4adc-be9c-76bec92f43b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "test_x_vectors = vectorizer.transform(test_x_sr)\n",
    "\n",
    "print(f\"Test set score: {np.mean([clf_svm.predict(test_x_vectors) == np.array(test_y)])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
